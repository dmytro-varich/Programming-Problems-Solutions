{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1 - Autograd PyTorch"
      ],
      "metadata": {
        "id": "W6qyvUsZxhse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4uPgxBdkwvKa"
      },
      "outputs": [],
      "source": [
        "# The autograd package provides automatic differentiation\n",
        "# for all operations on Tensors\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad = True -> tracks all operations on the tensor\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQP2SQvHx-FE",
        "outputId": "ca1ee7d9-05fc-44f8-da03-22d833c030b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.8321,  2.0750, -1.2427], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable gradient tracking for tensor x\n",
        "# x.requires_grad_(True)\n",
        "\n",
        "# Define a new tensor y as a result of an operation on x\n",
        "y = x + 2\n",
        "\n",
        "# Since y is created as a result of an operation, it has a grad_fn attribute\n",
        "# grad_fn references a Function that has created the tensoe\n",
        "print(f\"x -> {x}\")  # created by the user -> grad_fn is None\n",
        "print(f\"y -> {y}\")\n",
        "\n",
        "# Perform more operations on y\n",
        "z = y * y * 3\n",
        "print(f\"z -> {z}\")\n",
        "\n",
        "# Calculate the mean of z\n",
        "z = z.mean()\n",
        "print(f\"z sum -> {z}\")\n",
        "\n",
        "# Perform backpropagation to compute the gradients\n",
        "# When computation finishes, call .backward() to compute gradients automatically\n",
        "# The gradient for this tensor wil be accumulates into .grad attribute\n",
        "# It represents the partial derivative of the function with respect to the tensor\n",
        "z.backward()\n",
        "print(f\"x grad -> {x.grad}\")\n",
        "\n",
        "# Detach tensor x from the computational graph to demonstrate example of accumulating gradient\n",
        "x = x.detach()\n",
        "print(x.detach())\n",
        "print(f\"Is gradient calculation required? {x.requires_grad}\")\n",
        "# Generally speaking, torch.autograd is engine for computing vector-Jacobian product\n",
        "# It computes partial derivatives while applying the chain rule"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCVvAPPyNIE",
        "outputId": "fd79ae57-4304-4cab-d15e-b1d1ffc5591c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x -> tensor([ 1.8321,  2.0750, -1.2427], requires_grad=True)\n",
            "y -> tensor([3.8321, 4.0750, 0.7573], grad_fn=<AddBackward0>)\n",
            "z -> tensor([44.0559, 49.8157,  1.7204], grad_fn=<MulBackward0>)\n",
            "z sum -> 31.86400604248047\n",
            "x grad -> tensor([7.6643, 8.1499, 1.5146])\n",
            "tensor([ 1.8321,  2.0750, -1.2427])\n",
            "Is gradient calculation required? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor 'x' of shape (3,) with requires_grad set to Trye to enable gradient tracking\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "# Perform a series of operations on 'x' to obtain tensor 'y'\n",
        "# 'y' will have the same shape as 'x' and will contain the result of the operations\n",
        "y = x * 2  # Multiply 'x' by 2\n",
        "for _ in range(10):\n",
        "  y = y * 2  # Multiply 'y' by repeatedly, resulting in a non-scalar output\n",
        "\n",
        "# Print the tensor 'y' and its shape\n",
        "print(y)\n",
        "print(y.shape)\n",
        "\n",
        "# Create a tensor 'v' representing the gradient with respect to 'y'\n",
        "# This tensor specifies how much each element of 'y' contributes to the final gradient\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
        "\n",
        "# Use backward() with an additional 'gradient' argument to compute gradients of non-scalar outputs\n",
        "# 'v' acts as the gradient argument here, indicating the gradients that should be backpropagated\n",
        "y.backward(v)\n",
        "\n",
        "# Print the gradients of 'x' after backpropagation\n",
        "print(x.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYBwffss19uR",
        "outputId": "363abb0c-3be1-4142-adc6-cd78e2d4ffd1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1188.9368,  1335.0835,   259.0704], grad_fn=<MulBackward0>)\n",
            "torch.Size([3])\n",
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if requires_grad is enabled for tensor 'a'\n",
        "a = torch.randn(2, 2)\n",
        "print(a.requires_grad)\n",
        "\n",
        "# Perform operations on tensor 'a' to create tensor 'b'\n",
        "b = ((a * 2) / (a - 1))\n",
        "\n",
        "# Print the gradient function of tensor 'b'\n",
        "# Since 'b' is a result of operations on 'a', it will have a gradient function\n",
        "print(b.grad_fn)\n",
        "\n",
        "# Enable requires_grad for tensor 'a' using requires_grad_()\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "\n",
        "# Perform operations on tensor 'a' to create tensor 'b'\n",
        "b = (a * a).sum()\n",
        "\n",
        "# Print the gradient function of tensor 'b'\n",
        "# Since 'b' is a result of operations on 'a', it will have a gradient function\n",
        "print(b.grad_fn)\n",
        "\n",
        "# Detach tensor 'a' to create tensor 'b' without gradient computation\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad)\n",
        "\n",
        "b = a.detach()\n",
        "print(b.requires_grad)\n",
        "\n",
        "# Wrap the operation in 'with torch.no_grad()' to temporarily disable gradient tracking\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad)\n",
        "with torch.no_grad():\n",
        "  print((a ** 2).requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zaeYBNCF2k",
        "outputId": "c2e58505-1517-4474-8548-593784a9c0ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "None\n",
            "True\n",
            "<SumBackward0 object at 0x7b20cf3b6b30>\n",
            "True\n",
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final expamle"
      ],
      "metadata": {
        "id": "NgukAObvEs0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor 'weights' with requires_grad enabled for optimization\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "# Run training for multiple epochs\n",
        "for epoch in range(3):\n",
        "  # Just a dummy example: compute model output (sum of weights multiplied by 3)\n",
        "  model_output = (weights * 3).sum()\n",
        "\n",
        "  # Perform backpropagation to compute gradients\n",
        "  model_output.backward()\n",
        "\n",
        "  # Print the gradients of weights\n",
        "  print(weights.grad)\n",
        "\n",
        "  # Optimize the model by adjusting weights using gradient descent\n",
        "  # Update weights using gradient descent formula: new_weights = old_weights - leraning_rate * gradient\n",
        "  with torch.no_grad():\n",
        "    weights -= 0.1 * weights.grad\n",
        "\n",
        "  # Important step: Empty the gradients before the next optimization step to avoid accumulation\n",
        "  weights.grad.zero_()\n",
        "\n",
        "# After training, print the final weights and model output\n",
        "print(weights)\n",
        "print(model_output)\n",
        "\n",
        "# Note: Optimizers provided by torch.optim automatically handle gradient updates and zeroing gradients\n",
        "# Example usage:\n",
        "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
        "# During training loop:\n",
        "# optimizer.step()  # Update weights based on gradients\n",
        "# optimizer.zero_grad()  # Clear gradients for the next iteration"
      ],
      "metadata": {
        "id": "i2ixXvO1EmQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}